{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "import yaml\n",
    "\n",
    "\n",
    "# point this toward your JAVA bin\n",
    "os.environ['JAVAHOME'] = 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_144\\\\bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set TensorFlow Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape((1, -1))\n",
    "    max_x = np.max(x, axis=1).reshape((-1, 1))\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\n",
    "\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data Parameters\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", r\"runs\\1523831946\\checkpoints\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "#print(\"\\nParameters:\")\n",
    "#for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#    print(\"{}={}\".format(attr.upper(), value))\n",
    "#print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tweets for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "### Twitter API Auth Info.\n",
    "\n",
    "#Enter your Twitter API Authentication info below \n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "def get_user_tweets(query, count,include_rts=True):\n",
    "    \n",
    "    api = tweepy.API(auth_handler=auth, wait_on_rate_limit = True , wait_on_rate_limit_notify = True)\n",
    "    \n",
    "    try:\n",
    "        results = api.user_timeline(screen_name = query, count = count, include_rts = include_rts)\n",
    "        return(results)\n",
    "    except tweepy.TweepError as e:\n",
    "        print(e)\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_tweets = get_user_tweets('unsaifi',count=100)\n",
    "raw_tweets = []\n",
    "for i in user_tweets:\n",
    "    json = i._json\n",
    "    raw_tweets.append(json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS: Load data. Load your own data here\n",
    "dataset_name = cfg[\"datasets\"][\"default\"]\n",
    "datasets = {\"target_names\" : [i for i in os.listdir('data/input/SentenceCorpus/')]}\n",
    "x_raw = raw_tweets\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Saved TensorFlow Model to Predict Labels for Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Saifi\\Py_Projects\\Applied_AI\\cnn-text-classification-tf-cahya\\runs\\1523831946\\checkpoints\\model-200\n",
      "\n",
      "*********************** Evaluation Complete ***********************\n"
     ]
    }
   ],
   "source": [
    "# Map data into vocabulary\n",
    "vocab_path = r\"runs\\1523831946\\checkpoints\\..\\vocab\"\n",
    "#vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(r\"runs\\1523831946\\checkpoints\")\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        scores = graph.get_operation_by_name(\"output/scores\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), 64, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "        all_probabilities = None\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions_scores = sess.run([predictions, scores], {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions_scores[0]])\n",
    "            probabilities = softmax(batch_predictions_scores[1])\n",
    "            if all_probabilities is not None:\n",
    "                all_probabilities = np.concatenate([all_probabilities, probabilities])\n",
    "            else:\n",
    "                all_probabilities = probabilities\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "    print(metrics.classification_report(y_test, all_predictions, target_names=datasets['target_names']))\n",
    "    print(metrics.confusion_matrix(y_test, all_predictions))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw),\n",
    "                                              [int(prediction) for prediction in all_predictions],\n",
    "                                              [ \"{}\".format(probability) for probability in all_probabilities]))\n",
    "#out_path = os.path.join(\"prediction.csv\")\n",
    "#print(\"Saving evaluation to {0}\".format(out_path))#\n",
    "#with open(out_path, 'w') as f:\n",
    "#    csv.writer(f).writerows(predictions_human_readable)\n",
    "\n",
    "print('\\n*********************** Evaluation Complete ***********************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have a look at the predicted categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions_human_readable,columns=['text','pred_label_number','probs']).drop(labels='probs',axis=1)\n",
    "df['pred_label_text'] = df['pred_label_number'].map(lambda x: datasets['target_names'][int(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred_label_number</th>\n",
       "      <th>pred_label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @SkySportsPL: WATCH: Rivals for 22 years, f...</td>\n",
       "      <td>3</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @FaseehMangi: There are about 10,000 solar ...</td>\n",
       "      <td>8</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @charliebilello: $10,000 invested in the Am...</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @FaseehMangi: As global prices drop, solar ...</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @spectatorindex: BREAKING: South Korea will...</td>\n",
       "      <td>5</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @Channel4News: \"He's put up with the most a...</td>\n",
       "      <td>7</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @Arsenal: Legend.\\n\\n#MerciArsène https://t...</td>\n",
       "      <td>7</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @AP: BREAKING: President Trump: U.S. strike...</td>\n",
       "      <td>5</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @business: China's billions are set to revi...</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @MaxCRoser: Why numbers are sometimes bette...</td>\n",
       "      <td>3</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text pred_label_number  \\\n",
       "0  RT @SkySportsPL: WATCH: Rivals for 22 years, f...                 3   \n",
       "1  RT @FaseehMangi: There are about 10,000 solar ...                 8   \n",
       "2  RT @charliebilello: $10,000 invested in the Am...                 1   \n",
       "3  RT @FaseehMangi: As global prices drop, solar ...                 1   \n",
       "4  RT @spectatorindex: BREAKING: South Korea will...                 5   \n",
       "5  RT @Channel4News: \"He's put up with the most a...                 7   \n",
       "6  RT @Arsenal: Legend.\\n\\n#MerciArsène https://t...                 7   \n",
       "7  RT @AP: BREAKING: President Trump: U.S. strike...                 5   \n",
       "8  RT @business: China's billions are set to revi...                 1   \n",
       "9  RT @MaxCRoser: Why numbers are sometimes bette...                 3   \n",
       "\n",
       "  pred_label_text  \n",
       "0   entertainment  \n",
       "1            tech  \n",
       "2        business  \n",
       "3        business  \n",
       "4        politics  \n",
       "5           sport  \n",
       "6           sport  \n",
       "7        politics  \n",
       "8        business  \n",
       "9   entertainment  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            35\n",
       "politics         21\n",
       "tech             17\n",
       "business         13\n",
       "entertainment     9\n",
       "tennis            4\n",
       "Name: pred_label_text, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred_label_text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (Stanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Change the path according to your system\n",
    "stanford_classifier = r'C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\Scripts\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz'\n",
    "stanford_ner_path = r'C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\Scripts\\stanford-ner\\stanford-ner.jar'\n",
    "java_path = r\"C:\\Program Files\\Java\\jre1.8.0_144\\bin\" # replace this\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# Creating Tagger Object\n",
    "st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "\n",
    "\n",
    "def custom_NER(text):\n",
    "    \n",
    "    text = text.replace(\"’\",'')\n",
    "    text = text.strip()\n",
    "    tokenized_tweet = word_tokenize(text)\n",
    "    classified_text = [st.tag(tokenized_tweet)]\n",
    "    \n",
    "    person  = []\n",
    "    location = []\n",
    "    org = []\n",
    "    \n",
    "    try:\n",
    "        for ct in classified_text:\n",
    "            for i in range(1,len(ct) - 1):\n",
    "                \n",
    "                if (ct[i][1] == 'PERSON' and ct[i+1][1] == 'PERSON') or (ct[i][1] == 'LOCATION' and ct[i+1][1] == 'LOCATION') or (ct[i][1] == 'ORGANIZATION' and ct[i+1][1] == 'ORGANIZATION'):\n",
    "                    joined_word = ct[i][0] + ' ' + ct[i+1][0]\n",
    "                    if ct[i][1] == 'PERSON':\n",
    "                        person.append(joined_word)\n",
    "                    elif ct[i][1] == 'LOCATION':\n",
    "                        location.append(joined_word)\n",
    "                    else:\n",
    "                        org.append(joined_word)\n",
    "\n",
    "                elif (ct[i][1] == 'PERSON' and ct[i+1][1] != 'PERSON' and ct[i-1][1] != 'PERSON') or (ct[i][1] == 'LOCATION' and ct[i+1][1] != 'LOCATION' and ct[i-1][1] != 'LOCATION') or (ct[i][1] == 'ORGANIZATION' and ct[i+1][1] != 'ORGANIZATION' and ct[i-1][1] != 'ORGANIZATION'):\n",
    "                    joined_word = ct[i][0]\n",
    "                    if ct[i][1] == 'PERSON':\n",
    "                        person.append(joined_word)\n",
    "                    elif ct[i][1] == 'LOCATION':\n",
    "                        location.append(joined_word)\n",
    "                    else:\n",
    "                        org.append(joined_word)\n",
    "    \n",
    "        result = {'person' : person,\n",
    "                 'location' : location,\n",
    "                 'org' : org}\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Most Frequent People, Organizations & Locations mentioned in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_results_by_label = []\n",
    "\n",
    "for category in df['pred_label_text'].unique():\n",
    "    sub_df = df[df['pred_label_text'] == category]\n",
    "    \n",
    "    \n",
    "    for tweet in sub_df['text'].values:\n",
    "        ner_result = custom_NER(tweet)\n",
    "        ner_result['category'] = category\n",
    "        ner_results_by_label.append(ner_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NER = pd.DataFrame(ner_results_by_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NER_counts = []\n",
    "\n",
    "for type_of_NER in ['location','org','person']:\n",
    "    res = pd.DataFrame()\n",
    "    res = df_NER.set_index(['category'])[type_of_NER].apply(pd.Series).stack()\n",
    "    res = res.reset_index()\n",
    "    res.drop(labels='level_1',axis=1,inplace=True)\n",
    "    res.rename({0:type_of_NER},axis=1,inplace=True)\n",
    "    for i in res.to_dict('records'):\n",
    "        NER_counts.append(i)\n",
    "\n",
    "    #print(\"\\nMost Common \", type_of_NER)    \n",
    "    #for cat in res['category'].unique():\n",
    "    #    print('Top 5 in ', cat)\n",
    "    #    print(res[res['category'] == 'sport'][type_of_NER].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Locations Mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan        12\n",
      "China            3\n",
      "Saudi Arabia     2\n",
      "U.S.             2\n",
      "North Korea      1\n",
      "Name: location, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if len(pd.DataFrame(NER_counts)['location'].value_counts()) < 5:\n",
    "    print(pd.DataFrame(NER_counts)['location'].value_counts())\n",
    "else:\n",
    "    print(pd.DataFrame(NER_counts)['location'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Organizations Mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google              2\n",
      "Republican Party    1\n",
      "Amazon IPO          1\n",
      "Roy Moore           1\n",
      "School Peshawar     1\n",
      "Name: org, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if len(pd.DataFrame(NER_counts)['org'].value_counts()) < 5:\n",
    "    print(pd.DataFrame(NER_counts)['org'].value_counts())\n",
    "else:\n",
    "    print(pd.DataFrame(NER_counts)['org'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Frequent People Mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malala         5\n",
      "FaseehMangi    3\n",
      "Ive            2\n",
      "BarackObama    2\n",
      "Yves Tanguy    1\n",
      "Name: person, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if len(pd.DataFrame(NER_counts)['person'].value_counts()) < 5:\n",
    "    print(pd.DataFrame(NER_counts)['person'].value_counts())\n",
    "else:\n",
    "    print(pd.DataFrame(NER_counts)['person'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Top 5 by Predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NER_counts = pd.DataFrame(NER_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category :  entertainment \n",
      "\n",
      "Diwali      1\n",
      "Pakistan    1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "Manchester United    1\n",
      "LUMS                 1\n",
      "Name: org, dtype: int64 \n",
      "\n",
      "Asma Jahangir    1\n",
      "Arsene Wenger    1\n",
      "Malala           1\n",
      "Name: person, dtype: int64 \n",
      "\n",
      "\n",
      "Category :  tech \n",
      "\n",
      "Boston    1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "Google      2\n",
      "McKinsey    1\n",
      "AT &        1\n",
      "& amp       1\n",
      "IHC         1\n",
      "Name: org, dtype: int64 \n",
      "\n",
      "Letterman         1\n",
      "Malala            1\n",
      "S. Fischer        1\n",
      "Fischer Verlag    1\n",
      "Dave              1\n",
      "Name: person, dtype: int64 \n",
      "\n",
      "\n",
      "Category :  business \n",
      "\n",
      "Pakistan          3\n",
      "China             2\n",
      "Michigan State    1\n",
      "Asia              1\n",
      "Cambodia          1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "USA Gymnastics         1\n",
      "MacDonalds Szechuan    1\n",
      "Harvard                1\n",
      "Amazon IPO             1\n",
      "Bloomberg              1\n",
      "Name: org, dtype: int64 \n",
      "\n",
      "FaseehMangi     2\n",
      "Bitcoin         1\n",
      "Larry Nassar    1\n",
      "Kim Jong-Un     1\n",
      "Name: person, dtype: int64 \n",
      "\n",
      "\n",
      "Category :  politics \n",
      "\n",
      "Pakistan        3\n",
      "Saudi Arabia    2\n",
      "North Korea     1\n",
      "South Korea     1\n",
      "Syria           1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "of Politics        1\n",
      "School Peshawar    1\n",
      "Senate             1\n",
      "Public School      1\n",
      "Oxford Uni         1\n",
      "Name: org, dtype: int64 \n",
      "\n",
      "Hillary Clinton    1\n",
      "Chag Sameach       1\n",
      "Malala             1\n",
      "Muhammad Iqbal     1\n",
      "Obama              1\n",
      "Name: person, dtype: int64 \n",
      "\n",
      "\n",
      "Category :  sport \n",
      "\n",
      "Pakistan         5\n",
      "EST Space        1\n",
      "Space Shuttle    1\n",
      "Man City         1\n",
      "China            1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "Republican Party    1\n",
      "Cazorla             1\n",
      "Plant Roy           1\n",
      "Roy Moore           1\n",
      "Arsenal             1\n",
      "Name: org, dtype: int64 \n",
      "\n",
      "Ive               2\n",
      "Malala            2\n",
      "FaseehMangi       1\n",
      "Michelle          1\n",
      "FarrukhKPitafi    1\n",
      "Name: person, dtype: int64 \n",
      "\n",
      "\n",
      "Category :  tennis \n",
      "\n",
      "Series([], Name: location, dtype: int64) \n",
      "\n",
      "Series([], Name: org, dtype: int64) \n",
      "\n",
      "Coquelin    1\n",
      "Name: person, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df_NER_counts['category'].unique():\n",
    "    \n",
    "    print('\\nCategory : ', i,'\\n')\n",
    "    \n",
    "    sub_df = df_NER_counts[df_NER_counts['category'] == i]\n",
    "    \n",
    "    for j in ['location','org','person']:\n",
    "        if len(sub_df[j].value_counts()) < 5:\n",
    "            print(sub_df[j].value_counts(),'\\n')\n",
    "        else:\n",
    "            print(sub_df[j].value_counts()[:5],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
